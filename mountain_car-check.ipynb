{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy \n",
    "The policy takes into consideration the position of the car with respect to the flag and its velocity.\n",
    "state 1 gives the velocity of the car. If its value is positive, then the car is driven towards the flag, otherwise the car is pushed to the left side, away from the flag. state 0 gives the postion of the car. If the car's position is greatrer than a certain threshold value, then the car is pushed away from the flag, otherwise it is driven towards the flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    if state[1] < 0:\n",
    "        action = 0\n",
    "    elif state[1] > 0:\n",
    "            action = 2\n",
    "    elif state[0] > -1:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = 2\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards obtained : [ -87. -157.  -95. -159. -159. -139. -154. -154.  -94. -153. -159. -158.\n",
      " -100.  -87. -155.  -88.  -89. -152. -156.  -89.]\n",
      "Maximum reward obtained -87.0 Minimum reward obtained  -159.0 Mean reward obtained  -129.2\n"
     ]
    }
   ],
   "source": [
    "reward_total=[]\n",
    "for i_episode in range(20):\n",
    "    state = env.reset()\n",
    "\n",
    "    count=0\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        \n",
    "        \n",
    "        action=policy(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        count=count+reward\n",
    "        if done:\n",
    "            #print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "\n",
    "            break\n",
    "    reward_total.append(count)\n",
    "\n",
    "reward_total=np.array(reward_total)\n",
    "print(\"Rewards obtained :\",reward_total)\n",
    "print(\"Maximum reward obtained\",np.max(reward_total),\"Minimum reward obtained \" ,np.min(reward_total),\"Mean reward obtained \" ,np.mean(reward_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
