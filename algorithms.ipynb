{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uninformed searches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1,Dfs : Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. \n",
    "        The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and \n",
    "        explores as far as possible along each branch before backtracking.\n",
    "2,Bfs: Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. \n",
    "       It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key'[1]), \n",
    "       and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.\n",
    "3,Ucs:\n",
    "       Uniform cost search is a tree search algorithm that determines a path to the goal state that has the lowest weight.\n",
    "       It explores the nodes based on their costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informed search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Heuristic: Problem specific knowledge that (tries to) lead the search algorithm\n",
    "           faster towards a goal state. Often implemented via heuristic function h(n).\n",
    "       \n",
    "\n",
    "A*search :\n",
    "            g(n): Cost of reaching node n from initial node\n",
    "            h(n): Estimated cost from node n to nearest goal\n",
    "                  \n",
    "                        A* evaluation function: f(n) = g(n) + h(n)\n",
    "                     → f(n) is estimated cost of cheapest solution through n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dijkstra's algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "It is an algorithm for finding the shortest paths between nodes in a graph.We maintain two sets, one set contains vertices included in shortest path tree,\n",
    "other set includes vertices not yet included in shortest path tree. \n",
    "At every step of the algorithm, we find a vertex which is in the other set (set of not yet included) and has a minimum distance from the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial search :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Minimax : Minimax is a kind of backtracking algorithm that is used in decision making and game theory to find the optimal move for a player, \n",
    "          assuming that your opponent also plays optimally.\n",
    "          In Minimax the two players are called maximizer and minimizer. \n",
    "          The maximizer tries to get the highest score possible while the minimizer tries to do the opposite and get the lowest score possible.\n",
    "\n",
    "Expectimax:In addition to \"min\" and \"max\" nodes of the traditional minimax tree, this variant has \"chance\" (\"move by nature\") nodes, \n",
    "           which take the expected value of a random event occurring.\n",
    "           Instead of taking the max or min of the utility values of their children, chance nodes take a weighted average, \n",
    "           with the weight being the probability that child is reached.\n",
    "\n",
    "Alpha-beta pruning: \n",
    "                  Alpha–beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree.\n",
    "                  It stops completely evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move.\n",
    "                  When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov decision process and Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Value iteration:\n",
    "Value iteration is a method of computing an optimal MDP policy and its value.\n",
    "Value iteration starts at the \"end\" and then works backward, refining an estimate of either Q* or V*. There is really no end, so it uses an arbitrary end point.\n",
    "\n",
    "policy iteration: \n",
    "Policy iteration starts with a policy and iteratively improves it. \n",
    "It starts with an arbitrary policy π0 (an approximation to the optimal policy works best) and carries out the following steps \n",
    "starting from i=0.\n",
    "\n",
    "Q-learning :\n",
    "The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. \n",
    "It does not require a model of the environment and can handle problems with stochastic transitions and rewards, without requiring adaptations.\n",
    "Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy.\n",
    "\"Q\" names the function that returns the reward used to provide the reinforcement and can be said to stand for the \"quality\" of an action taken in a given state.\n",
    "\n",
    "Cross entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
